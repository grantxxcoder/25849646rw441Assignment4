\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{booktabs}
\usepackage[font=footnotesize]{subcaption}

% *** START DOCUMENT ***
\begin{document}

% Title Section
\title{Assignment 4: Random Forest Analysis of Tree Size}

\author{
\IEEEauthorblockN{Grant Booysen 25849646}
\IEEEauthorblockA{Email: 25849646@sun.ac.za}
}

\maketitle

% Abstract

\begin{abstract}
The report investigates how random forest performance varies with decision-tree depth, the number of randomly selected features per 
split, and ensemble size across three classification datasets of increasing complexity. The goal is to track the performance from 
underfitting with very shallow trees and few features to overfitting with deep trees and many features, identify points 
that maximize generalization, and characterize interactions among these hyperparameters. Experiments with (i) maximum depth from minimal to deep trees, (ii) number of features to 
split on with depth fixed at the best value from (i), and (iii) the balance number of trees with depth were conducted. A final experiment 
creates ensembles that mix under- and overfitting trees. Models are evaluated with train/test accuracy and 5-fold cross-validation. Findings 
are consistent: increased depth improves test accuracy up to a point, after which overfitting degrades or stagnates 
performance and the optimal depth increases with problem complexity. With depth fixed, increased number of features per split
 improves performance up to a plateau where medium complexity favors moderate feature subsets, while complex data benefits from larger subsets. 
 The inclusion of more trees in the ensemble improves stability and accuracy up to a plateau, with gains moderated by tree depth. 
 Bagging with only overfitted trees proved to have either the best stability, cross validation accuracy or test accuracy across all three datasets. 

  Overall, the best configurations balance depth, feature subsampling, and ensemble size where larger values are generally preferred as 
  task complexity rises. 
\end{abstract}


% Keywords (optional)
\begin{IEEEkeywords}
Random Forest, Ensemble learning, Bagging, Decision Trees
\end{IEEEkeywords}


\section{Introduction}

Random forests are widely used for classification due to strong generalization, robustness, and modest tuning requirements. The random forest model's  
performance is controlled by key hyperparameters that trade off bias and variance such as: maximum decision-tree depth, the number of 
randomly selected features per split, and ensemble size. The report aims to provide understanding for how these controls move the model to better 
performance. The idea is to identify configurations that generalize across problem complexities.

The goal of this study is to characterize how tree depth and feature subsampling influence random forest performance, and to identify 
configurations that maximize test accuracy and cross-validated performance across datasets of increasing difficulty. The analysis is achieved by: 
(i) exploring maximum depth from minimal to deep trees at fixed ensemble size, number of random features per split, and bagging; (ii) fixing 
depth at the best value from (i) and exploring the optimal number of features to split on per node; (iii) exploring depth and ensemble-size 
interactions; and (iv) composing ensembles that intentionally mix underfitted and overfitted trees. Evaluation uses train/test accuracy and 
5-fold cross-validation to better assess generalization and stability

The report aims to provide a reproducible behaviour for the bias-variance trade-off for random forests, and to extract 
generalizations for hyperparameter selection that adapts to dataset complexity. The design follows controlled experiments to reveal 
how each hyperparameter influences performance, and how they interact. The analysis spans three synthetic datasets of increasing complexity to
assess how optimal configurations shift with task difficulty.

The main observations are consistent with the bias-variance expectations:
- Increasing the depth of the trees in the ensemble improves test accuracy up to a point, after which gains stagnate or degrade; the optimal 
depth increases with complexity, for example, approximately 8, 15, and 20 on the simple, medium, and complex datasets respectively.
- With depth fixed, increasing the number of features to split on improves performance then plateaus; moderate subsets suit medium complexity,
 while larger subsets benefit complex data.
- Adding trees improves stability and accuracy up to a plateau, where the number of trees needed to reach the plateau is dependent on the depth 
of the trees.
- Mixed-depth and overfit-only ensembles: on the simple dataset, majority underfitted trees achieved the highest test accuracy of 0.967 while 
only overfitted attained the best cross-validated score of 0.966; on the medium dataset, a balanced mix yielded the highest test accuracy 
of 0.683 while only overfitted achieved the best cross-validated score of 0.691; on the complex dataset, only overfitted trees achieved the highest test
 accuracy of 0.582 and the most stable cross-validation (CV), with majority overfitted marginally higher CV mean of 0.548.

The remainder of the report is organized as follows: Section~2 provides background; Section~3 details the methodology and implementation; 
Section~4 describes the empirical procedure; Section~5 presents results and discussion; Section~6 concludes with implications and future directions.

\section{Background}
The background section discusses the fundamental concepts and algorithms used in the report, including decision trees, ensemble learning, and 
random forests.

\subsection{Decision Trees}
The decision tree is a supervised learning algorithm used for classification and regression tasks. The decision tree works by recursively 
splitting the data into subsets based on feature values, creating a tree-like model of decisions. Each internal node represents a condition on a 
feature, each branch represents the outcome of the test, and each leaf node represents a class label in classification or a continuous value 
in regression. Decision trees are easy to interpret and visualize, but they can be prone to overfitting, especially with deep trees.

\subsection{Ensemble Learning: Bagging}
Bagging, or Bootstrap Aggregating, is an ensemble learning technique that aims to improve the stability and accuracy of machine learning algorithms.
 The algorithm works by training multiple models on different subsets of the training data, which are created by randomly sampling with 
 replacement known as bootstrapping. The final prediction is made by aggregating the predictions of all individual models, typically by 
 averaging for regression or majority voting for classification. Bagging helps reduce variance and combat overfitting, making the approach 
 particularly effective for high-variance models like decision trees.

\subsection{Random Forests}

Random forests are an ensemble learning method that combines multiple decision trees to improve predictive performance and control 
overfitting. The key idea is to leverage the diversity of individual trees by training them on different subsets of the data and features. Traditionally,
random forests use bagging to create diverse training sets for each tree. In random forests, it is expected that each tree should be grown
to the maximum depth and overfit. The high variance of each tree is then averaged out by the ensemble, leading to a final model that generalizes well.

\begin{algorithm}[H]
\caption{Random Forest (high-level pseudocode)}
\begin{algorithmic}[1]
\REQUIRE Training data $(X, y)$, n\_estimators, max\_depth, max\_features, bootstrap size
\FOR{each tree $t = 1 \dots$ n\_estimators}
  \STATE Draw a bootstrap sample from $(X, y)$  % bagging
  \STATE Grow a decision tree with depth limit max\_depth
  \STATE At each node, consider max\_features randomly selected features
\ENDFOR
\STATE For prediction, average class probabilities (or majority vote) across trees
\end{algorithmic}
\end{algorithm}


\section{Implementation}
The implementation section describes the datasets, data preparation, and the random forest model used in the experiments.

\subsection{Data preparation}
Three synthetic, tabular classification datasets of increasing complexity were programmatically generated using 
\texttt{make\_classification} from \texttt{sklearn.datasets} (\cite{scikit-learn}) to enable controlled bias-variance studies and
 avoid confounds from missing values or categorical encoding. A single \texttt{random\_state} ensured reproducibility for generation and splitting.

\textbf{Datasets (generation intent and parameters):}
\begin{itemize}
  \item Simple: binary classification, low noise, no overlap between classes, \texttt{n\_samples=500}, \texttt{n\_features=10} where eight are informative and two are redundant, \texttt{class\_sep=2.0}, \texttt{flip\_y=0.0}. Designed to saturate quickly with shallow trees and show limited overfitting.
  \item Medium: three classes, moderate overlap, mild noise, \texttt{n\_samples=2000}, \texttt{n\_features=25} where 18 are informative and five are redundant, \texttt{class\_sep=1.0}, \texttt{flip\_y=0.2}. Targets visible bias-variance trade-offs at medium depths.
  \item Complex: four classes, many features, higher noise, \texttt{n\_samples=6000}, \texttt{n\_features=50} where 35 are informative and ten are redundant, \texttt{class\_sep=0.7}, \texttt{flip\_y=0.3}. Encourages deeper trees and progressive overfitting.
\end{itemize}

\textbf{Train/test split:} A stratified split with \texttt{test\_size=0.30} and fixed \texttt{random\_state} preserves 
class proportions across splits and guarantees identical partitions across runs.


\textbf{Preprocessing:} No scaling or normalization is applied as decision trees are insensitive to monotonic feature transforms. No imputation is
 required as the synthetic data contain no missing values. No label encoding is needed as the labels are already integers.

\textbf{Depth ranges for experimental control:} Per-dataset depth grids are predefined to bound sweeps and reflect expected capacity needs:
\begin{itemize}
  \item Simple: \{1, 2, 3, 4, 5, 6, 8, 10, 12\}
  \item Medium: \{1, 3, 5, 7, 10, 12, 15, 18, 20, 25, 30\}
  \item Complex: \{1, 3, 5, 7, 10, 12, 15, 20, 25, 30, 35, 40\}
\end{itemize}

\subsection{Random forest model}


\subsection{Training and cross-validation procedure}




% 4. Empirical Procedure

\section{Empirical Procedure}
The empirical procedure involved the following steps: model construction with hyperparameter tuning, training and validation, and finally testing and evaluation. Each step is described in detail in the following subsections.

\subsection{Pipeline and hyperparameter tuning}
Each dataset required different preprocessing steps for the hyperparameter tuning. The method for tuning is described in this subsection.

\paragraph{Iris dataset hyperparameter tuning}
The Iris dataset was small enough to directly apply the hyperparameter tuning on the full training and validation sets. The hyperparameters that were tuned included the number of hidden layers, the number of 
neurons per layer, learning rate, and activation function. The following configurations were used for the baseline hyperparameter tuning over 300 epochs:
\begin{itemize}
\item Hidden layers: [1, 2]
\item Neurons per layer: [1, 2, 4, 8]
\item Learning rate: [0.001, 0.005, 0.01, 0.5, 0.1]
\item Activation function: [ReLU, Tanh]
\end{itemize}
 The following configurations were used for the incremental class learner hyperparameter tuning over 1000 epochs:
\begin{itemize}
\item Moving average window size: [1, 2, 3, 5, 10, 20]
\item Plateau threshold: [0.004, 0.01, 0.05, 0.1]
\item Underfit threshold: [0.1, 0.2, 0.3, 0.4, 0.5]
\item Overfit threshold (training): [0.1, 0.15, 0.2]
\item Overfit threshold (validation): [0.1, 0.15, 0.2]
\item Minimum number of neurons: [1, 2, 4, 8]
\item Learning rate: [0.001, 0.005, 0.01, 0.5, 0.1]
\item Activation function: [ReLU, Tanh]
\item Maximum hidden layers: [1, 2, 3]
\end{itemize}
Each configuration was run three times with different random seeds; however, the results were the same across the runs due to 
the small size of the dataset. The best configuration was selected based on the highest validation accuracy.
\paragraph{Fashion-MNIST dataset hyperparameter tuning}
 The Fashion-MNIST dataset required a smaller stratified sample of the training and validation sets to perform the hyperparameter tuning. Specifically, 10\% of the training and validation set was used for the Fashion-MNIST hyperparameter tuning. The baseline tuning required a minimum of 300 epochs to 
 converge to a good solution for the sample data. 
The following configurations were used for the baseline hyperparameter tuning over 300 epochs:
\begin{itemize}
\item Hidden layers: [1, 2, 3, 4]
\item Neurons per layer: [50, 64, 100, 128, 150, 200, 256, 300, 512]
\item Learning rate: [0.001, 0.005, 0.5, 0.1]
\item Activation function: [ReLU, Tanh]
\end{itemize}
 The incremental class learner made use of the same configurations as the Iris dataset; however, the maximum 
 number of hidden layers was increased to [1, 2, 3, 4]. Each configuration was run three times with
 different random seeds, starting from 42 to 44. The best configuration was selected based on the highest validation accuracy.

\subsection{Training and validation}
Each model was trained on the full training set and validation set. The training and validation losses and accuracies were recorded 
at each epoch to monitor the training process.  The models were cross-validated over 30 independent runs with different random seeds starting from 42 and incrementing by one per run. Over each run, the training and validation sets 
were ensured to be stratified to maintain the class distribution. The best model was selected based on the highest validation accuracy. The final 
test set was only used for the final evaluation of the models. 

\paragraph{Iris model training}
 Each Iris dataset baseline model was trained for a maximum of 150 epochs with early stopping using a patience of 10 epochs. 
Each incremental class learning model was trained for a maximum of 1000 epochs.
\paragraph{Fashion-MNIST model training}
 Each Fashion-MNIST dataset baseline model was trained for a maximum of 350 epochs with early stopping using a patience of 25 epochs. Each incremental 
 model was trained for a maximum of 3500 epochs.

\subsection{Metric comparison and statistical tests}
The following empirical procedure was used to determine whether the observed differences in test metrics (accuracy and F1) between the baseline and incremental models are statistically significant.

\begin{enumerate}
  \item Collect paired metric values across the repeated experiments where over 30 independent runs record the baseline and incremental values. 
  \item Visual inspection: produce paired boxplots of the differences to check for obvious non-normality or extreme outliers.
  \item Normality check: apply the Shapiro--Wilk test to the sample of paired differences for each metric using significance level \(\alpha=0.05\). Record the test statistic and p-value.
  \item Test selection:
    \begin{itemize}
      \item If the Shapiro--Wilk p-value \(\ge 0.05\), assume approximate normality and perform a two-sided paired Student's t-test on the differences.
      \item If the Shapiro--Wilk p-value \(< 0.05\), reject normality and perform a two-sided Wilcoxon signed-rank test on the paired differences.
    \end{itemize}
  \item Report results for each metric: test statistic, p-value, and decision (reject/fail to reject $H_0$ at \(\alpha=0.05\)).
\end{enumerate}

% 5. Research Results

\section{Research Results}
The results of the models are presented in the following section. The results include the hyperparameter tuning, training, validation, and
 cross-validation results, and finally, the statistical tests to determine if there was a significant difference between the two models.

\subsection{Iris dataset hyperparameter tuning results}
 The top configurations were found to have a single hidden layer with few neurons, a high learning rate of 0.1, and the ReLU activation function. To keep the model simple, the configuration with 4 neurons was chosen as the final model. 
 The configurations and their corresponding mean accuracy are shown in Table~\ref{tab:baseline_iris_best_configs}.
\begin{table}[H]
  \centering
  \begin{minipage}{0.85\linewidth}
    \centering
    \caption{Best Baseline Model Hyperparameter Configurations for Iris}
    \label{tab:baseline_iris_best_configs}
    \scriptsize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}p{0.19\linewidth} p{0.31\linewidth} p{0.15\linewidth} p{0.15\linewidth} p{0.15\linewidth}@{}}
      \toprule
      \shortstack{Hidden\\Layers} &
      \shortstack{Number of Neurons\\(layer 1, layer 2, \dots)} &
      \shortstack{Learning\\Rate} &
      \shortstack{Activation\\Function} &
      \shortstack{Accuracy\\(mean)} \\
      \midrule
 1 & (8,)   & 0.10  & ReLU & 0.977 $\pm$ 0.000\\
 1 & (4,)   & 0.10  & ReLU & 0.977 $\pm$ 0.000\\
 2 & (1,1)  & 0.001 & ReLU & 0.622 $\pm$ 0.000\\
 1 & (2,)   & 0.10  & ReLU & 0.611 $\pm$ 0.000\\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}
The optimal hyperparameter values for the incremental class learning model can be found in Table~\ref{tab:icl_iris_best_configs}. 
\begin{table}[H]
  \centering
  \caption{Best Incremental Class Learning Model Hyperparameter Configurations for Iris}
  \label{tab:icl_iris_best_configs}
  \scriptsize
  \setlength{\tabcolsep}{6pt}

  % First sub-table: training thresholds
  \begin{minipage}{\linewidth}
    \centering
    \begin{tabular}{@{}ccccc@{}}
      \toprule
 Moving & Plateau & Underfit & Overfit (Training) & Overfit (Validation) \\
 Average & Threshold & Threshold & Threshold & Threshold \\
 Window & & & & \\
      \midrule
 3 & 0.004 & 0.4 & 0.2 & 0.2 \\
 3 & 0.004 & 0.3 & 0.1 & 0.1 \\
 3 & 0.004 & 0.4 & 0.2 & 0.1 \\
 3 & 0.004 & 0.2 & 0.2 & 0.2 \\
      \bottomrule
    \end{tabular}
  \end{minipage}

  \vspace{6pt}

  % Second sub-table: architecture and results
  \begin{minipage}{\linewidth}
    \centering
    \begin{tabular}{@{}cccc@{}}
      \toprule
 Min Neurons & Learning Rate & Activation Function & Accuracy (mean) \\
      \midrule
 1 & 0.5 & ReLU & 0.9773 $\pm$ 0.0001\\
 8 & 0.5 & Tanh & 0.9772 $\pm$ 0.0001\\
 8 & 0.5 & ReLU & 0.9772 $\pm$ 0.0001\\
 8 & 0.5 & Tanh & 0.9500 $\pm$ 0.0001\\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}
A particular point to be made from looking at the results in Table~\ref{tab:icl_iris_best_configs} is the high learning rate of 0.5. 
The high learning rate is likely due to the small size of the dataset and the simplicity of the model. The high learning rate allows the model to converge quickly to a good solution.

\subsection{Fashion-MNIST dataset hyperparameter tuning results}
 The top configurations were found to have three or four hidden layers with many neurons, a low 
 learning rate of 0.001, and the ReLU activation function. The configurations and their 
 corresponding mean validation accuracy is shown in Table~\ref{tab:baseline_fashion_mnist_best_configs}. 
\begin{table}[H]
  \centering
  \begin{minipage}{0.85\linewidth}
    \centering
    \caption{Best Baseline Model Hyperparameter Configurations for Fashion-MNIST}
    \label{tab:baseline_fashion_mnist_best_configs}
    \scriptsize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}p{0.19\linewidth} p{0.31\linewidth} p{0.15\linewidth} p{0.15\linewidth} p{0.15\linewidth}@{}}
      \toprule
      \shortstack{Hidden\\Layers} &
      \shortstack{Number of Neurons\\(layer 1, layer 2, \dots)} &
      \shortstack{Learning\\Rate} &
      \shortstack{Activation\\Function} &
      \shortstack{Accuracy\\(mean)} \\
      \midrule
      4 & (512, 256, 128, 64)   & 0.001  & ReLU & 0.8860 $\pm$ 0.0054 \\
      4 & (300, 200, 100, 50)   & 0.001  & ReLU & 0.8833 $\pm$ 0.0067 \\
      3 & (512, 512, 512)  & 0.001 & ReLU & 0.8817 $\pm$ 0.0075 \\
      3 & (256, 128, 64)   & 0.001  & ReLU & 0.8816 $\pm$ 0.0076 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}
The optimal parameter values for the incremental class learning model indicate that adding more complexity past three hidden layers to the model was 
not very beneficial, with only a minimal increase in accuracy. The simplest model was thus chosen as the final model. 
The optimal hyperparameter values for the incremental class learning model can be found in Table~\ref{tab:icl_fashion_best_configs}. 
\begin{table}[H]
  \centering
  \caption{Best Incremental Class Learning Model Hyperparameter Configurations for Fashion-MNIST}
  \label{tab:icl_fashion_best_configs}
  \scriptsize
  \setlength{\tabcolsep}{6pt}

  % First sub-table: training thresholds
  \begin{minipage}{\linewidth}
    \centering
    \begin{tabular}{@{}ccccc@{}}
      \toprule
 Moving & Plateau & Underfit & Overfit (Training) & Overfit (Validation) \\
 Average & Threshold & Threshold & Threshold & Threshold \\
 Window & & & & \\
      \midrule
 10 & 0.001 & 0.5 & 0.1 & 0.15 \\
 10 & 0.001 & 0.5 & 0.15 & 0.1 \\
 10 & 0.001 & 0.5 & 0.15 & 0.15 \\
 10 & 0.001 & 0.4 & 0.1 & 0.1 \\
      \bottomrule
    \end{tabular}
  \end{minipage}

  \vspace{6pt}

  % Second sub-table: architecture and results
  \begin{minipage}{\linewidth}
    \centering
    \begin{tabular}{@{}cccc@{}}
      \toprule
 Min Neurons & Learning Rate & Activation Function & Accuracy (mean) \\
      \midrule
 16 & 0.01 & ReLU & 0.9374 $\pm$ 0.001\\
 16 & 0.01 & ReLU & 0.9374 $\pm$ 0.001\\
 16 & 0.01 & ReLU & 0.9374 $\pm$ 0.001\\
 16 & 0.01 & ReLU & 0.9277 $\pm$ 0.002\\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}
The best-performing incremental setup on Fashion-MNIST reached a mean accuracy of 0.9374 with 16 minimum neurons, 
a 0.01 learning rate, and ReLU activations. The top-performing models had similar configurations for the 
parameters, with the focus on the balance of underfitting and overfitting thresholds to ensure that the model was not 
too complex or too simple.

\subsection{Iris model training and validation results}
The learning curves for the best run of the baseline and incremental class learning models are shown in Figure~\ref{fig:iris_baseline_learning_curves}. 
The difference in the two loss curves highlights the difference in learning approaches, where the baseline 
model's loss progressively decreases until stabilizing, whilst the incremental learner first stabilizes the 
loss for the first two minority classes, and when adding the next class, there is a spike in loss, which is then smoothed and stabilized.
From the below learning curves, it can be seen that the incremental class learning model had 
sufficiently reduced the training and validation losses to a point where the model was not underfitting or overfitting. Although the final loss 
appears to be quite low for both training and validation, keep in mind that the dataset is small and thus has limited 
exposure to all the variability in the data.
% \begin{figure}[H]
%   \centering
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/iris_baseline_learning_curves_loss.pdf}
%     \caption{Training and validation loss for baseline Iris model}
%     \label{fig:iris_baseline_loss}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/iris_dynamic_learning_curves_loss.pdf}
%     \caption{Training and validation loss for dynamic Iris model}
%     \label{fig:iris_dyn_loss}
%   \end{subfigure}
%   \caption{Iris baseline model learning curves for the best run.}
%   \label{fig:iris_baseline_learning_curves}
% \end{figure}

 The cross-validation and test results provide a better indication of the model's performance and can be seen in Table~\ref{tab:metric_means_iris}.
The results below indicate that the baseline model outperformed the incremental class learning model on all metrics. The incremental class learning 
model struggled with the small dataset and was unable to learn the relationships in the data. The incremental model's underwhelming performance is likely due 
to the small size of the dataset. The baseline model performed better than the incremental class learning model, as it was able to consider the 
entire dataset at once and thus could learn the more nuanced relationships between classes better. The incremental class learning model was at a disadvantage as it had to learn the classes
progressively and thus had limited exposure to the data at any given time. Furthermore, since the validation set was small, the model 
would not be able to compute reliable trends to determine when to add complexity or new classes. The incremental model evidently struggled to 
improve its performance as it consistently predicted the same incorrect outputs as seen in the lack of variance in the results.

\begin{table}[H]
  \centering
  \caption{Mean $\pm$ std.\ dev.\ for Baseline and Incremental models. Percentage metrics shown as mean $\pm$ std (percent); time shown in seconds.}
  \label{tab:metric_means_iris}
  \scriptsize
  \setlength{\tabcolsep}{6pt}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{@{}lcc@{}}
    \toprule
 Metric & Baseline (Mean $\pm$ Std) & Incremental (Mean $\pm$ Std) \\
    \midrule
 Accuracy (Test)        & 96.21\% $\pm$ 2.17\% & 44.07\% $\pm$ 0.00\% \\
 F1 (Test)              & 96.87\% $\pm$ 1.78\% & 20.39\% $\pm$ 0.00\% \\
 Precision (Test)       & 97.16\% $\pm$ 1.73\% & 14.69\% $\pm$ 0.00\% \\
 Recall (Test)          & 96.79\% $\pm$ 1.72\% & 33.33\% $\pm$ 0.00\% \\
 Training Time (s)      & 0.376 $\pm$ 0.008 s   & 0.271 $\pm$ 0.004 s \\
 Accuracy (Train)       & 98.58\% $\pm$ 2.05\% & 32.00\% $\pm$ 0.00\% \\
 F1 (Train)             & 96.58\% $\pm$ 2.07\% & 13.01\% $\pm$ 0.00\% \\
 Precision (Train)      & 96.64\% $\pm$ 2.01\% & 8.08\%  $\pm$ 0.00\% \\
 Recall (Train)         & 96.58\% $\pm$ 2.12\% & 33.33\% $\pm$ 0.00\% \\
 Accuracy (Validation)  & 98.00\% $\pm$ 2.52\% & 32.00\% $\pm$ 0.00\% \\
 F1 (Validation)        & 98.12\% $\pm$ 2.38\% & 16.16\% $\pm$ 0.00\% \\
 Precision (Validation) & 98.35\% $\pm$ 2.05\% & 10.67\% $\pm$ 0.00\% \\
 Recall (Validation)    & 98.14\% $\pm$ 2.37\% & 33.33\% $\pm$ 0.00\% \\
      \bottomrule
      \end{tabular}%
 }
  \vspace{2pt}
  \footnotesize Note: Percentage metrics: means and standard deviations computed from raw decimals, then multiplied by 100 and shown with two decimal places. Time reported in seconds.
\end{table}

\subsection{Fashion-MNIST training and validation results}
Fortunately, the Fashion-MNIST dataset was large enough for the incremental class learning model to learn the relationships in the data. The results are thus more 
promising than that of the Iris dataset.
The learning curves for the best run of the baseline and incremental class learning models are shown in Figure~\ref{fig:fashion_baseline_learning_curves}. 
The baseline model's loss curves in Figure~\ref{fig:fashion_baseline_loss} show a typical training pattern where the training loss decreases and stabilizes,
 while the validation loss decreases and stabilizes at a higher loss. The incremental class learning model's loss curves in Figure~\ref{fig:fashions_dyn_loss} show a more complex pattern due to 
 the dynamic nature of the model. The loss curves show spikes in loss, indicative of either a new class being added to the training and validation set 
or a new hidden layer being added. Due to the strict thresholds for overfitting and underfitting, the loss curves for 
the dynamic model seemingly lie on top of each other. The scale of the fluctuations is much smaller compared to the baseline model, where the difference in training and validation loss is not as pronounced. To better visualize the results of the training of the dynamic 
Fashion-MNIST model, refer to the accuracies obtained as seen in Figure~\ref{fig:fashions_dyn_acc}.

% \begin{figure}[H]
%   \centering
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/fashion-mnist_baseline_learning_curves_loss.pdf}
%     \caption{Training and validation loss for baseline Fashion model}
%     \label{fig:fashion_baseline_loss}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/fashion_dynamic_learning_curves_loss.pdf}
%     \caption{Training and validation loss for the dynamic Fashion model}
%     \label{fig:fashions_dyn_loss}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{images/fashion_dynamic_learning_curves_acc.pdf}
%     \caption{Training and validation accuracy for the dynamic Fashion model}
%     \label{fig:fashions_dyn_acc}
%   \end{subfigure}
%   \caption{Fashion baseline model learning curves for the best run.}
%   \label{fig:fashion_baseline_learning_curves}
% \end{figure}

A particular point of interest is the first 100 epochs of Figure~\ref{fig:fashions_dyn_acc}, which highlight the initial learning dynamics. The validation accuracy oscillates 
between extremes, achieving incredibly high and low accuracies. The oscillation is due to the following: The model initially being simple memorizes the 
training data, achieves high training accuracy, and low validation accuracy. The model training process recognizes that result as overfitting, thus first tries 
to add complexity if the minimum number of neurons was not reached. The model then adds complexity and improves the validation performance, and achieves very high validation results as the data is still small. The model then adds 
a new class when a sufficiently low validation loss is reached. The process then repeats itself. The training only begins to stabilize when the model 
is complex enough to start learning relationships instead of memorizing the data. \\

The results of the cross validation and test results for the Fashion-MNIST dataset were more promising than that of the Iris dataset. 
The results can be found in Table~\ref{tab:metric_means_fashion}. As is seen from the results in Table~\ref{tab:metric_means_fashion}, the incremental class learning model outperformed the baseline model on most metrics; however, the 
 test metrics were very similar. The incremental learner had more stable performance as the standard deviations were lower across all metrics. A particular point 
 of interest is the training times, where the incremental learner took significantly longer to train than the baseline model. The trade-off between the two models 
 is between performance and training time, as the incremental class learning model took significantly longer to tune, train, and validate; however, it achieved more stable performance. 

\begin{table}[H]
  \centering
  \caption{Mean $\pm$ std.\ dev.\ for Baseline and Incremental models. Percentage metrics shown as mean $\pm$ std (percent); time shown in seconds.}
  \label{tab:metric_means_fashion}
  \scriptsize
  \setlength{\tabcolsep}{6pt}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{@{}lcc@{}}
    \toprule
 Metric & Baseline (Mean $\pm$ Std) & Incremental (Mean $\pm$ Std) \\
      \midrule
 Accuracy (Test)        & 83.59\% $\pm$ 7.04\% & 83.77\% $\pm$ 5.74\% \\
 F1 (Test)              & 82.96\% $\pm$ 8.18\% & 83.73\% $\pm$ 6.33\% \\
 Precision (Test)       & 83.87\% $\pm$ 7.70\% & 83.54\% $\pm$ 7.45\% \\
 Recall (Test)          & 83.55\% $\pm$ 6.93\% & 84.35\% $\pm$ 4.43\% \\
 Training Time (s)      & 9.65 $\pm$ 1.88 s    & 115.77 $\pm$ 12.70 s \\
 Accuracy (Train)       & 88.88\% $\pm$ 9.79\% & 93.94\% $\pm$ 8.26\% \\
 F1 (Train)             & 88.53\% $\pm$ 10.66\% & 94.29\% $\pm$ 7.93\% \\
 Precision (Train)      & 89.72\% $\pm$ 9.43\% & 93.84\% $\pm$ 9.27\% \\
 Recall (Train)         & 89.00\% $\pm$ 9.57\% & 95.25\% $\pm$ 5.36\% \\
 Accuracy (Validation)  & 86.89\% $\pm$ 8.81\% & 90.21\% $\pm$ 7.36\% \\
 F1 (Validation)        & 86.31\% $\pm$ 9.79\% & 90.29\% $\pm$ 7.42\% \\
 Precision (Validation) & 87.24\% $\pm$ 9.12\% & 89.98\% $\pm$ 8.71\% \\
 Recall (Validation)    & 86.76\% $\pm$ 8.66\% & 91.07\% $\pm$ 5.09\% \\
        \bottomrule
        \end{tabular}%
 }
  \vspace{2pt}
  \footnotesize Note: Percentage metrics: means and standard deviations computed from raw decimals, then multiplied by 100 and shown with two decimal places. Time reported in seconds.
\end{table}
The test metrics were very similar, thus requires further analysis to determine if the models were statistically different. 
The box plots in Figure~\ref{fig:boxplots} show the distributions of the test accuracy and F1 metrics across the 30 cross-validation runs. The box plot was 
used for the primary metrics of interest, accuracy and F1, as they provide a good visual representation of the distribution of the metrics.
Both boxplot figures show the same outlier performance in the incremental model. The baseline model had a higher median 
performance across both metrics and higher maximum values. The baseline performance is skewed to the left, where the 
distribution of the values is gathered at the higher scores. The results suggest that the baseline model is 
generally, a stronger performer than the incremental model; however, the baseline model had more variability in its performance. The statistical tests will confirm whether there 
is any statistical difference between the two models. 
% \begin{figure}[H]
%   \centering
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[height=5cm,keepaspectratio]{images/test_acc.pdf}
%     \caption{Test accuracy distributions (30 runs)}
%     \label{fig:box_accuracy}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{\linewidth}
%     \centering
%     \includegraphics[height=5cm,keepaspectratio]{images/test_f1.pdf}
%     \caption{Test F1 distributions (30 runs)}
%     \label{fig:box_f1}
%   \end{subfigure}
%   \caption{Box plots comparing the baseline and incremental models across cross-validation runs for Fashion-MNIST. Boxes show median, interquartile range, and outliers for each model.}
%   \label{fig:boxplots}
% \end{figure}


\paragraph{Per-class breakdown (Fashion-MNIST test)}
The per-class breakdown of the test results will better expose how the model performed on the minority classes on a particular run. The 
class-level F1s in Table~\ref{tab:fashion_per_class_f1} show that the dynamic model's main gain appears on Class 3, rather than exclusively on the 
minority classes in the training distribution. In other words, the incremental learner unfortunately did not simply “boost the small classes”. 

\begin{table}[H]
  \centering
  \caption{Per-class F1 on Fashion-MNIST test. Delta is (Dynamic $-$ Baseline). The largest improvement is on Class 3.}
  \label{tab:fashion_per_class_f1}
  \scriptsize
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{@{}lccc@{}}
    \toprule
 Class & F1 (Baseline) & F1 (Dynamic) & $\Delta$ \\
    \midrule
 0 & 0.93 & 0.94 & +0.01 \\
 1 & 0.95 & 0.95 & +0.00 \\
 2 & 0.92 & 0.94 & +0.02 \\
    \textbf{3} & \textbf{0.62} & \textbf{0.65} & \textbf{+0.03} \\
 4 & 0.94 & 0.95 & +0.01 \\
 5 & 0.77 & 0.77 & +0.00 \\
 6 & 0.87 & 0.86 & $-$0.01 \\
 7 & 0.77 & 0.78 & +0.01 \\
 8 & 0.97 & 0.97 & +0.00 \\
 9 & 0.81 & 0.80 & $-$0.01 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Data-efficiency findings (Fashion-MNIST)}
 Using random seeds 42 to 46 and a target validation accuracy of 92\%, the incremental learner
 reached the threshold with a smaller fraction of the training data than the baseline network. In other words, as the data budget increased, 
 the incremental curve crossed 0.92 earlier, whereas the baseline required a higher percentage to meet the same criterion. The behaviour 
matched the staged, trend-based training. By focusing capacity on the early classes and expanding only when losses plateau or change, the 
incremental model learns useful patterns sooner. The data efficiency graph can be seen in Figure~\ref{fig:data_efficiency}.
% \begin{figure}[H]
%   \centering
%     \includegraphics[width=\linewidth]{images/fashion-mnist:_baseline_vs_dynamic_accuracy_quick_plot.pdf}
%     \caption{Data efficiency comparison for Fashion-MNIST. Each point is the maximum validation accuracy over four random seeds at that training percentage. The horizontal dashed line marks the 92\% target.}
%     \label{fig:data_efficiency}
% \end{figure}
Over the four random seeds, both models dipped in performance at 50\% of the training data. The dip is likely due to the 
 particular samples chosen for that percentage, as the samples were stratified, but not guaranteed to be representative of the full dataset. 
 The incremental model reached the 92\% target at 40\% of the training data, whereas the baseline model required 60\% of the training data
 to reach the same target in the best run. The incremental model was thus more data efficient than the baseline model.
\paragraph{Statistical Significance Testing}

Shapiro-Wilk tests were used to assess the normality of the paired differences between the two models. The results
 for the tests were:
\begin{itemize}
  \item Shapiro-Wilk p-value for accuracy difference equals 0.0195; Shapiro-Wilk p-value for F1 difference equals 0.0204.
  \item Both p-values are less than 0.05, so the null hypothesis of normality is rejected for both differences.
\end{itemize}


Hypotheses (for each metric):
$H_0$: no difference between models; $H_1$: difference between models.

Because normality was rejected, Wilcoxon signed-rank tests were used:
\begin{table}[H]
  \centering
  \caption{Normality and Wilcoxon test results}
  \label{tab:stats_tests}
  \begin{tabular}{@{}l c c@{}}
    \toprule
 Test (metric) & Statistic & p-value \\
    \midrule
 Shapiro-Wilk (Accuracy diff) & -- & 0.0195 (reject normality) \\
 Shapiro-Wilk (F1 diff)       & -- & 0.0204 (reject normality) \\
 Wilcoxon (Accuracy)          & 215.0000 & 0.7188 (fail to reject $H_0$) \\
 Wilcoxon (F1)                & 228.0000 & 0.9262 (fail to reject $H_0$) \\
    \bottomrule
  \end{tabular}
\end{table}

Both paired differences deviate from normality, so nonparametric Wilcoxon tests were used, and in
 both cases fail to reject the null hypothesis, as no significant difference was detected between the models for
 accuracy or F1.

  \paragraph{Discussion}
The statistical tests indicate that there was no statistically significant difference between the two models for both accuracy and F1 metrics. Therefore, in the case of the Fashion-MNIST dataset, 
the incremental class learning model does not provide a statistically significant improvement over the baseline model in pure evaluation metrics. However, despite the 
lack of statistical significance, the incremental class learning model showed major improvement on the Fashion-MNIST dataset as compared to the Iris 
dataset. The poor performance on the Iris dataset was due to the small size of the dataset, where inter-class relationships 
could not be learned effectively. The architecture of the model favoured more aggressive learning, where the model would 
quickly add complexity to learn the relationships in the data. The aggressive learning was not suitable for the small Iris 
dataset, where the model would quickly overfit to the small amount of data. The incremental class learning model was more suited to the
 larger Fashion-MNIST dataset, where the model could learn the relationships in the data better. The incremental class learning model was 
 more stable, with lower standard deviations across all metrics. The improvement in stability was surprising as the incremental class learning model had more hyperparameters to tune, which would typically lead to more variability in the results.\\
 
 The incremental class learning model was also more data efficient, reaching 
 a target validation accuracy of 92\% with only 40\% of the training data, whereas the baseline model required 60\% of the training data
 to reach the same target for the Fashion-MNIST dataset. The incremental model was not able to show significant improvement on the minority 
 classes, where the largest improvement was on Class 3, which was not a minority class. Despite the lack of significant improvement on the 
 minority classes, the ability of the model to be more data efficient is beneficial in scenarios where data is imbalanced, limited, or expensive to obtain.
 In such scenarios, the imbalanced data problem is often coupled with limited data, where the incremental class learning model could be beneficial.

 Overall, the incremental class learning model shows promise as a learning approach if data efficiency and stability are priorities. The challenge 
 with the dynamic model was the hyperparameter tuning, where the model had many more parameters to tune than the baseline. Secondly, the dynamic model required 
 significantly more time and resources to train and validate, as the model took over 12 times longer to train than the baseline model. However, due to the flexibility of the approach, 
 the model could be adapted to different datasets and learning scenarios. 

\section{Conclusion}
The intention of the report was to determine the benefit of making use of incremental learning in neural networks to combat imbalanced datasets. Two datasets were used to evaluate the performance of the incremental class learning model against a baseline model. The Iris dataset was too small for the incremental class learning model to be effective, where the baseline model outperformed it on all metrics. 
The Fashion-MNIST dataset was more promising, where the incremental class learning model achieved higher scores on all metrics. Despite these metrics being higher, the models 
did not have a statistically significant difference in the accuracy and F1 scores. Despite the lack of statistical significance, the incremental class learning model was more data efficient, reaching a 
target validation accuracy of 92\% with only 40\% of the training data, whereas the baseline model required 60\% of the training data
 to reach the same target for the Fashion-MNIST dataset. The incremental class learning model was also more stable, with lower 
 standard deviations across all metrics. The trade-off between the two models is between performance and training time. The
 incremental class learning model took significantly longer to tune, train, and validate; however, it was able to achieve
 acceptable performance on fewer data observations than the baseline model. \\
\cite{blackard1999forest}.
 Further work could involve testing the models on larger and more complex datasets to better evaluate the performance of the incremental class learning model. The incremental class 
 learning model for the assignment was limited due to computational constraints, where the model could not be made too complex. With more computational resources, the model could be made deeper and wider to better learn the relationships in the data.
 The associated code for all the calculations
\footnote{The code for this report is available on GitHub: \url{https://github.com/grantxxcoder/25849646rw441Assignment4}.} can be found in the footnote and the references on the last page.
\appendix

\section*{List of Acronyms}
\begingroup
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.05}
\noindent\begin{tabular}{@{}p{0.22\columnwidth}p{0.75\columnwidth}@{}}
\textbf{Acronym} & \textbf{Definition} \\
\midrule
CV & Cross-validation \\
\end{tabular}
\endgroup

% References
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
